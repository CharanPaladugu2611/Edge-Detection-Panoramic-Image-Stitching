# -*- coding: utf-8 -*-
"""vpaladug_project2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VU8aAh4-tgV74HBT8ZQ5SsHBBHq0Id64

Problem-1: The algorithm uses canny edges to detect edges, houghlineP for line extraction.
"""

import cv2
import numpy as np
from google.colab import drive
from google.colab.patches import cv2_imshow
from IPython.display import display, Image
import matplotlib.pyplot as plt
drive.mount('/content/drive', force_remount=True)

"""I first imported all the necessary libraries and gave access to my google drive"""

def rmv_bg(frame):
    # Convert the frame to grayscale
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    # Threshold to create a binary mask
    _, mask = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY)

    # Invert the mask to keep the white regions
    mask_inv = cv2.bitwise_not(mask)

    # Bitwise AND the original frame with the inverted mask
    result = cv2.bitwise_and(frame, frame, mask=mask_inv)

    return result

"""The function removes the unwanted background using grayscale and creating a mask from the given frame."""

def fd_edgs(frame):
    # Convert the frame to grayscale
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    # Apply GaussianBlur to reduce noise and improve edge detection
    blurred = cv2.GaussianBlur(gray, (5, 5), 0)

    # Apply Canny edge detector
    edges = cv2.Canny(blurred, 400,550 )  # You can adjust the threshold values

    return edges

"""I then wrote a function to find edges in the given frame using canny edges."""

def fd_dom_lns(edges):
    # Apply Hough Transform to detect lines
    lines = cv2.HoughLinesP(edges, 1, np.pi / 180, threshold=50, minLineLength=50, maxLineGap=10)

    return lines

"""The function above depecits line extraction from the given frame using hough transform probability."""

def fltr_lns(lines, min_length):
    dominant_lines = []

    for line in lines:
        x1, y1, x2, y2 = line[0]
        length = np.sqrt((x2 - x1)**2 + (y2 - y1)**2)

        if length >= min_length:
            dominant_lines.append(line)

    return dominant_lines

"""Now fltr_lins is used to retain only the dominant lines."""

def comp_intersections(lines):
    intersections = []

    for i in range(len(lines)):
        for j in range(i + 1, len(lines)):
            line1 = lines[i][0]
            line2 = lines[j][0]

            x1, y1, x2, y2 = line1
            x3, y3, x4, y4 = line2

            # Compute intersection point
            det = (x1 - x2) * (y3 - y4) - (y1 - y2) * (x3 - x4)
            if det != 0:
                intersection_x = ((x1 * y2 - y1 * x2) * (x3 - x4) - (x1 - x2) * (x3 * y4 - y3 * x4)) / det
                intersection_y = ((x1 * y2 - y1 * x2) * (y3 - y4) - (y1 - y2) * (x3 * y4 - y3 * x4)) / det

                intersections.append((intersection_x, intersection_y))

    return intersections[:5]

"""This function is used to find the intersections of dominant lines."""

def mk_crns(frame, corners):
    # Convert BGR to RGB
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    # Draw red corners
    for corner in corners:
        x, y = map(int, corner)  # Convert corners to integers
        cv2.circle(frame_rgb, (x, y), 4, (255, 0,0), -1)

    frame_final=cv2.cvtColor(frame_rgb, cv2.COLOR_BGR2RGB)
    # Display the frame with Harris corners and blue boundary lines
    # display(Image(data=cv2.imencode('.png', frame_final)[1]))
    return frame_final

"""The function above draws circles on corners. It also has the code to verify using harris corner detector you just have to uncomment that line."""

def fltr_crns(corners, frame_width, frame_height):
    filtered_corners = []

    for corner in corners:
        x, y = corner

        # Check if the corner is within the frame boundaries
        if 0 <= x <= frame_width and 0 <= y <= frame_height:
            filtered_corners.append(corner)

    return filtered_corners

"""Now I wrote a function which filters out the uneccessary corners"""

def draw_lns(frame, lines):
    line_image = np.zeros_like(frame)

    for line in lines:
        x1, y1, x2, y2 = line
        cv2.line(line_image, (x1, y1), (x2, y2), (255, 0, 0), 5)

    result = cv2.addWeighted(frame, 1, line_image, 1, 0)

    return result

"""The function uses a different line image to draw lines on a specified frame."""

def comp_laplacian_var(frame):
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    # Define the Laplacian kernel
    laplacian_kernel = np.array([[0, 1, 0], [1, -4, 1], [0, 1, 0]], dtype=np.float32)

    # Apply the Laplacian filter using cv2.filter2D
    laplacian = cv2.filter2D(gray, cv2.CV_64F, laplacian_kernel)

    # Calculate the variance of the Laplacian
    variance = np.var(laplacian)

    return variance

"""Used to calculate variance using Laplacian kernel"""

def create_output_video(input_path, output_path):

    cap = cv2.VideoCapture(input_path)

    frame_width = int(cap.get(3))
    frame_height = int(cap.get(4))
    fps = cap.get(10)

    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, 10, (frame_width, frame_height))

    frame_count = 0
    skip_count = 0
    while True:
        ret, frame = cap.read()

        if not ret:
            break

        frame_count += 1

        variance = comp_laplacian_var(frame)
        threshold = 90

        if variance < threshold:
            skip_count += 1
            print(f"Skipped frame {frame_count} - Blurry (Variance: {variance})")
            continue

        segmented_frame = rmv_bg(frame)
        edges = fd_edgs(frame)
        lines = fd_dom_lns(edges)
        min_line_length = 100
        dominant_lines = fltr_lns(lines, min_line_length)
        intersections = comp_intersections(dominant_lines)
        filtered_corners = fltr_crns(intersections, frame_width, frame_height)
        frame_with_lines = draw_lns(frame.copy(), [line[0] for line in dominant_lines])
        frame_out=mk_crns(frame_with_lines, intersections)

        # Display the frame with overlay in Colab
        # cv2_imshow(frame_with_lines)

        # Write the frame with overlay to the output video
        out.write(frame_out)

    cap.release()
    out.release()
    cv2.destroyAllWindows()

    print(f"Total frames skipped: {skip_count}")
    print(f"Total frames: {frame_count}")

"""This function just call all the above created functions. This also prints out the number of frames and frames skipped as well."""

# Update the file paths with your Google Drive paths
input_video_path = '/content/drive/MyDrive/ENPM673/HW2/proj2_v2.mp4'
output_video_path = '/content/drive/MyDrive/ENPM673/HW2/output.mp4'
# Create the output video
create_output_video(input_video_path, output_video_path)

"""This is the main function.

---------------------------------------------------------------------------------------------------------------------------------------------------------------

Problem-2: Scale-Invariant Feature Transform, or SIFT for short, is a popular option for panoramic stitching because of its strong and adaptable features. Because of its scale invariance, it can effectively identify and match features across photos that have different object sizes, which solves a typical problem in panoramic scenes. Furthermore, precise feature matching is ensured by SIFT's rotation invariance, even in cases when photos record scenes from diverse viewpoints. The algorithm is quite good at extracting unique characteristics, which lowers the possibility of mismatching identical patterns or textures in complicated scenarios. Its accurate keypoint localization through the use of a Difference of Gaussians (DoG) method enhances feature recognition accuracy overall. SIFT performs well in a variety of lighting scenarios and situations thanks to its resilience to variations in illumination and its capacity to produce accurate and consistent feature descriptions.
"""

def ext_ftrs(image, method='sift'):
    # Check the feature extraction method specified and initialize the detector
    if method == 'sift':
        sift = cv2.SIFT_create()
        # Detect keypoints and compute descriptors using SIFT
        keypoints, descriptors = sift.detectAndCompute(image, None)
    elif method == 'orb':
        orb = cv2.ORB_create()
        # Detect keypoints and compute descriptors using ORB
        keypoints, descriptors = orb.detectAndCompute(image, None)
    # Add other methods if needed

    # Return the detected keypoints and computed descriptors
    return keypoints, descriptors

"""The function above extracts features"""

def mtch_ftrs(desc1, desc2, method='bf'):
    # Check the matching method specified and initialize the matcher
    if method == 'bf':
        bf = cv2.BFMatcher()
        # Perform knn matching using the Brute-Force Matcher
        matches = bf.knnMatch(desc1, desc2, k=2)
    # Add FLANN matcher or other methods if needed

    # Apply ratio test to filter out good matches
    best_match = []
    for m, n in matches:
        if m.distance < 0.75 * n.distance:
            best_match.append(m)

    # Return the filtered best matches
    return best_match

"""mtch_ftrs is used to match the features between the images"""

def comp_homo_man(kp1, kp2, matches, iterations=1000, threshold=5.0):
    # Initialize variables to store the best homography and corresponding inliers
    max_inliers = []
    best_homo = None

    # Iterate through a specified number of iterations
    for _ in range(iterations):
        # Randomly select 4 matches
        np.random.shuffle(matches)
        selected_matches = matches[:4]

        # Construct pairs of points for homography estimation
        src_pts = np.float32([kp1[m.queryIdx].pt for m in selected_matches]).reshape(-1, 1, 2)
        dst_pts = np.float32([kp2[m.trainIdx].pt for m in selected_matches]).reshape(-1, 1, 2)

        # Estimate homography using the Direct Linear Transform (DLT) method
        A = []
        for i in range(4):
            x, y = src_pts[i][0]
            x_prime, y_prime = dst_pts[i][0]
            A.append([-x, -y, -1, 0, 0, 0, x*x_prime, y*x_prime, x_prime])
            A.append([0, 0, 0, -x, -y, -1, x*y_prime, y*y_prime, y_prime])

        A = np.array(A)
        U, S, Vt = np.linalg.svd(A)
        H = Vt[-1].reshape(3, 3)

        # Apply homography and count inliers
        inliers = []
        for match in matches:
            pt1 = np.append(np.array(kp1[match.queryIdx].pt), 1)
            pt2 = np.array(kp2[match.trainIdx].pt)
            prj_pt1 = np.dot(H, pt1)

            # Normalize
            prj_pt1 = (prj_pt1 / prj_pt1[2])[:2]

            # Calculate error and check if the match is an inlier
            if np.linalg.norm(prj_pt1 - pt2) < threshold:
                inliers.append(match)

        # Update best model if the current model is better (more inliers)
        if len(inliers) > len(max_inliers):
            max_inliers = inliers
            best_homo = H

    # Return the best homography matrix
    return best_homo

"""I then wrote a function to compute the homography"""

def stitch_imgs(img1, img2, H):
    # Warp the perspective of img1 using the provided homography matrix H
    result = cv2.warpPerspective(img1, H, (img1.shape[1] + img2.shape[1], img1.shape[0]))

    # Place img2 onto the warped img1 to create the stitched result
    result[0:img2.shape[0], 0:img2.shape[1]] = img2

    # Return the final stitched image
    return result

def create_pano(images):
    # Initialize the current image with the first image in the list
    current_image = images[0]

    # Iterate through the remaining images in the list
    for i in range(1, len(images)):
        # Extract features (keypoints and descriptors) from the current image and the next image
        kp1, desc1 = ext_ftrs(current_image)
        kp2, desc2 = ext_ftrs(images[i])

        # Match features between the current image and the next image
        matches = mtch_ftrs(desc1, desc2)

        # Compute homography matrix using RANSAC to find a robust transformation
        H = comp_homo_man(kp1, kp2, matches)

        # Stitch the current image with the next image using the computed homography matrix
        current_image = stitch_imgs(current_image, images[i], H)

    # Return the final panoramic image
    return current_image

image_paths = ["/content/drive/MyDrive/ENPM673/HW2/PA120272.JPG", "/content/drive/MyDrive/ENPM673/HW2/PA120273.JPG", "/content/drive/MyDrive/ENPM673/HW2/PA120274.JPG", "/content/drive/MyDrive/ENPM673/HW2/PA120275.JPG"]
images = [cv2.imread(image_path) for image_path in image_paths]
pano = create_pano(images)
cv2_imshow(pano)

"""2)b)Creating a smooth, wide-angle panoramic view involves integrating many photographs, frequently taken from various views, a technique known as panoramic mosaicing. Capturing images, aligning them, mixing them, and maybe correcting distortion are the main processes in panoramic mosaicing. For panoramic mosaicing, it is preferable to rotate the camera at its centre because this rotation behaviour improves depth estimation, minimises stitching errors, aligns the optical centre, helps maintain parallax consistency, and works well with algorithms made expressly for this kind of rotation."""